"""
Seperate 2by2 3by3 ... extract purely CGR generated by granularity change
"""
import json
import pathlib
import collections
from match import get_commit_refdict, extract_coarse_grained_refs_oline_as_supportive, get_retraced_commit_refdict2
from utils import load_commit_pairs_all


def search_sub_commit_combination(l: list):
    """
    search consecutive substring (exclude itself)
    :param l:
    :return:
    """
    res = []
    for length in range(2, len(l)):
        for bias in range(0, len(l) + 1 - length):
            res.append(tuple(l[bias:bias + length]))
    return res


def revert_dict_key_value(d):
    new_d = dict()
    for each in d:
        new_d[str(d[each])] = each
    return new_d


def print_refs(coarse_grained_commit, normal_grained_commits, CGRs):
    print("*" * 30)
    print(f"coarse grained commits {coarse_grained_commit}")
    print(f"fine grained commits {normal_grained_commits}")
    for each in CGRs:
        print(each)
    print("*" * 30)


def write_refs2file(coarse_grained_commit, normal_grained_commits, CGRs, file):
    with open(file, "a") as f:
        f.write("*" * 30 + "\n")
        f.write(f"coarse grained commits {coarse_grained_commit}\n")
        f.write(f"fine grained commits {normal_grained_commits}\n")
        for each in CGRs:
            f.write(str(each) + "\n")
        f.write("*" * 30 + "\n")


def extract_CGR_no_filter_sub(repo_path):
    """
    Extract CGRs, and the CGRs are only determined by comparing and matching with normal-grained refactorings, in addition,
    the matching identifier is the signature of refactoring
    e.g. granularity level 5 CGR only needs to compare with
    :param repo_path:
    :return:
    """
    d = load_commit_pairs_all(repo_path)
    reverted_d = revert_dict_key_value(d)
    squash_units = [0] * 4
    effective_squash_units = [0] * 4

    all_CGR = list()

    for coarse_grained_commit in d.keys():
        normal_grained_commits = d[coarse_grained_commit]
        coarse_granularity = len(normal_grained_commits)
        squash_units[coarse_granularity - 2] = squash_units[coarse_granularity - 2] + 1

        squash_log_p = str(
            pathlib.Path(repo_path).joinpath(str(coarse_granularity)).joinpath(f"log{coarse_granularity}.txt"))

        # get normal grained refs

        normal_grained_refs = get_commit_refdict(squash_log_p, list(normal_grained_commits), coarse_grained_commit)

        # get coarse grained refs
        coarser_grained_refs = get_commit_refdict(squash_log_p, coarse_grained_commit)

        # CGRs only compare with normal grained refactorings
        CGRs = extract_coarse_grained_refs_oline_as_supportive(coarser_grained_refs, normal_grained_refs)

        if len(CGRs):
            effective_squash_units[coarse_granularity - 2] = effective_squash_units[coarse_granularity - 2] + 1
            all_CGR.append(CGRs)
            # print_refs(coarse_grained_commit, normal_grained_commits, CGRs)

    # print(squash_units)
    # print(effective_squash_units)
    # print([effective_squash_units[i] / squash_units[i] for i in range(0, 4)])

    return all_CGR, cal_effective_squash_units(repo_path.split("/")[-1].replace("_cr", ""), squash_units,
                                               effective_squash_units)


def extract_CGR_contained_CGC(repo_path):
    """
    Extract the coarse-grained commits that contains CGR
    The CGRs are determined by comparing and matching with lower-granularity CGR & normal-grained refactorings
    TODO: Most common logic is the same as method extract_CGR_filter_sub, should apply Extract Method ref
    :param repo_path:
    :return:
    """
    d = load_commit_pairs_all(repo_path)
    reverted_d = revert_dict_key_value(d)
    all_CGR = list()

    CGC_contain_CGR = []

    for coarse_grained_commit in d.keys():
        normal_grained_commits = d[coarse_grained_commit]
        coarse_granularity = len(normal_grained_commits)
        squash_log_p = str(
            pathlib.Path(repo_path).joinpath(str(coarse_granularity)).joinpath(f"log{coarse_granularity}.txt"))

        # get normal grained refs
        normal_grained_refs = get_commit_refdict(squash_log_p, list(normal_grained_commits), coarse_grained_commit)

        # get coarse grained refs
        coarser_grained_refs = get_commit_refdict(squash_log_p, coarse_grained_commit)

        ref_cgc_sub = []

        # coarse granularity 2 CGRs only compare with normal grained refactorings
        if coarse_granularity > 2:  # coarse granularity larger than 2 CGRs compare with normal & smaller coarse
            # granularity CGRs
            sub_commit_combinations = search_sub_commit_combination(normal_grained_commits)

            for combination in sub_commit_combinations:
                cgc_sub = reverted_d[str(combination)]
                ref_cgc_sub += get_retraced_commit_refdict2(squash_log_p,
                                                            cgc_sub,
                                                            search_list_index(normal_grained_commits,
                                                                              str(combination[-1])), len(combination))

        CGRs = extract_coarse_grained_refs_oline_as_supportive(coarser_grained_refs, normal_grained_refs + ref_cgc_sub)

        if len(CGRs):
            all_CGR.append(CGRs)
            CGC_contain_CGR.append(coarse_grained_commit)
    return CGC_contain_CGR


def extract_CGR_filter_sub(repo_path):
    """
    Extract granulrity level 2 to 5 CGRs from repository
    the extracted CGR are compared and matched with lower-granularity CGRs to exclude duplicate detection
    e.g. level 4 CGR should be compared and matched with normal-grained, 2~3 granularity CGR.
    :param repo_path:
    :return:
    """
    d = load_commit_pairs_all(repo_path)
    reverted_d = revert_dict_key_value(d)
    squash_units = [0] * 4
    effective_squash_units = [0] * 4

    all_CGR = list()
    CGR_count = {}

    for coarse_grained_commit in d.keys():
        normal_grained_commits = d[coarse_grained_commit]
        coarse_granularity = len(normal_grained_commits)
        squash_units[coarse_granularity - 2] = squash_units[coarse_granularity - 2] + 1
        if coarse_granularity not in CGR_count.keys():
            CGR_count[coarse_granularity] = 0

        squash_log_p = str(
            pathlib.Path(repo_path).joinpath(str(coarse_granularity)).joinpath(f"log{coarse_granularity}.txt"))

        # get normal grained refs
        normal_grained_refs = get_commit_refdict(squash_log_p, list(normal_grained_commits), coarse_grained_commit)

        # get coarse grained refs
        coarser_grained_refs = get_commit_refdict(squash_log_p, coarse_grained_commit)

        ref_cgc_sub = []

        # coarse granularity 2 CGRs only compare with normal grained refactorings
        if coarse_granularity > 2:  # coarse granularity larger than 2 CGRs compare with normal & smaller coarse
            # granularity CGRs
            sub_commit_combinations = search_sub_commit_combination(normal_grained_commits)

            for combination in sub_commit_combinations:
                cgc_sub = reverted_d[str(combination)]
                ref_cgc_sub += get_retraced_commit_refdict2(squash_log_p,
                                                            cgc_sub,
                                                            search_list_index(normal_grained_commits,
                                                                              str(combination[-1])), len(combination))

        CGRs = extract_coarse_grained_refs_oline_as_supportive(coarser_grained_refs, normal_grained_refs + ref_cgc_sub)

        if len(CGRs):
            effective_squash_units[coarse_granularity - 2] = effective_squash_units[coarse_granularity - 2] + 1
            all_CGR.append(CGRs)
            CGR_count[coarse_granularity] += len(CGRs)
            # print_refs(coarse_grained_commit, normal_grained_commits, CGRs)

            # for each in CGRs:
            #     if str(each) == "Move Methodretrolambda/src/main/java/net/orfjackal/retrolambda/LambdaUsageBackporter.java@public visitEnd() : voidretrolambda/src/main/java/net/orfjackal/retrolambda/LambdaClassBackporter.java@public visitEnd() : voidretrolambda/src/main/java/net/orfjackal/retrolambda/LambdaUsageBackporter.java 86:92":
            #         print("coarse granularity", coarse_granularity)
            #         print("coarse granied commit", coarse_grained_commit)

    # print(squash_units)
    # print(effective_squash_units)
    # print([effective_squash_units[i] / squash_units[i] for i in range(0, 4)])

    return all_CGR, cal_effective_squash_units(repo_path.split("/")[-1].replace("_cr", ""), squash_units,
                                               effective_squash_units), CGR_count


def search_list_index(ll, sl):
    """
    ll = (A,B,C,...), where B is the parent commit of A, and C is the parent commit of B.
    :param ll: the fine-grained commits
    :param sl: the target commit
    :return: the number of parent commits of the target commit in ll
    e.g. ll=(A,B,C), sl = C, the index of C is 2
    the return should be 3-1-2 = 0
    """
    return len(ll) - 1 - ll.index(sl)


def cal_effective_squash_units(repo_name, squash_units, effective_squash_units):
    if len(squash_units) != len(effective_squash_units) or len(squash_units) != 4:
        raise Exception(f"Input error: squash units: {squash_units}, effective squash units:{effective_squash_units}")
    res = {repo_name: {
        "2by2": effective_squash_units[0] / squash_units[0],
        "3by3": effective_squash_units[1] / squash_units[1],
        "4by4": effective_squash_units[2] / squash_units[2],
        "5by5": effective_squash_units[3] / squash_units[3]}}
    return res


def calculate_override_ratio(repo_path):
    def isOverride(s):
        if "@Override" in s:
            return True
        return False

    def isAddMethodAnnotation(s):
        if "Add Method Annotation" in s:
            return True
        return False

    """
    Calculate the ratio of @override appears in Add Method Annotation refactoring
    """
    d = load_commit_pairs_all(repo_path)
    reverted_d = revert_dict_key_value(d)
    all_CGR = list()
    ref_record = {"Add Method Annotation": 0, "@Override": 0}

    for coarse_grained_commit in d.keys():
        normal_grained_commits = d[coarse_grained_commit]
        coarse_granularity = len(normal_grained_commits)
        squash_log_p = str(
            pathlib.Path(repo_path).joinpath(str(coarse_granularity)).joinpath(f"log{coarse_granularity}.txt"))
        # get normal grained refs
        normal_grained_refs = get_commit_refdict(squash_log_p, list(normal_grained_commits), coarse_grained_commit)
        # get coarse grained refs
        coarser_grained_refs = get_commit_refdict(squash_log_p, coarse_grained_commit)
        ref_cgc_sub = []
        # coarse granularity 2 CGRs only compare with normal grained refactorings
        if coarse_granularity > 2:  # coarse granularity larger than 2 CGRs compare with normal & smaller coarse
            # granularity CGRs
            sub_commit_combinations = search_sub_commit_combination(normal_grained_commits)
            for combination in sub_commit_combinations:
                cgc_sub = reverted_d[str(combination)]
                ref_cgc_sub += get_retraced_commit_refdict2(squash_log_p,
                                                            cgc_sub,
                                                            search_list_index(normal_grained_commits,
                                                                              str(combination[-1])), len(combination))
        CGRs = extract_coarse_grained_refs_oline_as_supportive(coarser_grained_refs, normal_grained_refs + ref_cgc_sub)

        if len(CGRs):
            for each in CGRs:
                if isAddMethodAnnotation(each.type):
                    ref_record["Add Method Annotation"] += 1
                    if isOverride(str(each.left)) or isOverride(str(each.right)):
                        ref_record["@Override"] += 1
            all_CGR.append(CGRs)
            # print_refs(coarse_grained_commit, normal_grained_commits, CGRs)

    return ref_record


def override_ratio():
    data = {'jfinal_cr': {'Add Method Annotation': 0, '@Override': 0},
            'mbassador_cr': {'Add Method Annotation': 0, '@Override': 0},
            'javapoet_cr': {'Add Method Annotation': 0, '@Override': 0},
            'jeromq_cr': {'Add Method Annotation': 1, '@Override': 1},
            'seyren_cr': {'Add Method Annotation': 0, '@Override': 0},
            'retrolambda_cr': {'Add Method Annotation': 1, '@Override': 0},
            'baasbox_cr': {'Add Method Annotation': 0, '@Override': 0},
            'sshj_cr': {'Add Method Annotation': 0, '@Override': 0},
            'xabber-android_cr': {'Add Method Annotation': 10, '@Override': 10},
            'android-async-http_cr': {'Add Method Annotation': 9, '@Override': 5},
            'giraph_cr': {'Add Method Annotation': 1, '@Override': 0},
            'spring-data-rest_cr': {'Add Method Annotation': 1, '@Override': 0},
            'blueflood_cr': {'Add Method Annotation': 4, '@Override': 3},
            'HikariCP_cr': {'Add Method Annotation': 0, '@Override': 0},
            'redisson_cr': {'Add Method Annotation': 12, '@Override': 12},
            'goclipse_cr': {'Add Method Annotation': 24, '@Override': 22},
            'morphia_cr': {'Add Method Annotation': 25, '@Override': 6},
            'PocketHub_cr': {'Add Method Annotation': 3, '@Override': 3},
            'hydra_cr': {'Add Method Annotation': 10, '@Override': 8},
            'cascading_cr': {'Add Method Annotation': 23, '@Override': 20},
            'helios_cr': {'Add Method Annotation': 0, '@Override': 0},
            'RoboBinding_cr': {'Add Method Annotation': 44, '@Override': 44},
            'truth_cr': {'Add Method Annotation': 6, '@Override': 1},
            'rest.li_cr': {'Add Method Annotation': 4, '@Override': 0},
            'rest-assured_cr': {'Add Method Annotation': 6, '@Override': 0},
            'JGroups_cr': {'Add Method Annotation': 21, '@Override': 4},
            'processing_cr': {'Add Method Annotation': 166, '@Override': 165},
            'zuul_cr': {'Add Method Annotation': 5, '@Override': 4}}
    add_method_annotation_count = 0
    override_count = 0
    for repo in data:
        add_method_annotation_count += data[repo]['Add Method Annotation']
        override_count += data[repo]['@Override']
    print("Add Method Annotation ", add_method_annotation_count)
    print("override count ", override_count)
    print("overrider/add_method ", override_count / add_method_annotation_count)


def CGR_frequency_per_commit():
    """
    general frequency =  # of CGR/ # of commits
    frequency per granularity = # of CGR per granularity/ # of commits of that repo
    :return: frequency per granularity
    """
    total_CGR_count = 0
    total_commits_count = 0
    with open("dataset_info/CGR_count.json", "r") as f:
        cgr_count = json.load(f)
        total_CGR_count = sum(
            [cgr_count[repo]["2"] + cgr_count[repo]["3"] + cgr_count[repo]["4"] + cgr_count[repo]["5"] for repo in
             cgr_count])
        print("Total CGR count", total_CGR_count)

    with open("dataset_info/commit_count.json", "r") as f:
        commit_count = json.load(f)
        total_commits_count = sum([commit_count[repo]["commit_count"] for repo in commit_count])
        print("Total commits count", total_commits_count)

    print("# of CGR per normal commit", total_CGR_count / total_commits_count)

    normal_frequencies = {}
    for repo_cr in cgr_count:
        repo = repo_cr.split("_cr")[0]
        normal_frequencies[repo] = {}
        for granularity_level in cgr_count[repo_cr]:
            normal_frequencies[repo][granularity_level] = cgr_count[repo_cr][granularity_level] / commit_count[repo][
                "commit_count"]

    with open("RQ1_normal_frequency.json", "w") as f:
        json.dump(normal_frequencies, f)
    print("normal frequencies saved to RQ1_normal_frequency.json")
    return normal_frequencies


def CGR_frequency_per_refactoring():
    """
    # of CGR/ # of normal refactoring
    :return:
    """
    total_CGR_count = 0
    total_refactoring_count = 0
    with open("dataset_info/CGR_count.json", "r") as f:
        cgr_count = json.load(f)
        total_CGR_count = sum(
            [cgr_count[repo]["2"] + cgr_count[repo]["3"] + cgr_count[repo]["4"] + cgr_count[repo]["5"] for repo in
             cgr_count])
        print("Total CGR count", total_CGR_count)

    with open("/Users/leichen/Code/pythonProject/pythonProject/pythonProject/SCRMDetection/RQs/dataset_info"
              "/refactoring_number.json", "r") as f:
        refactoring_count = json.load(f)
        total_refactoring_count = sum([refactoring_count[repo]["1"] for repo in refactoring_count])
        print("Total refactorings count", total_refactoring_count)

    print("# of CGRs per refactoring", total_CGR_count / total_refactoring_count)


if __name__ == "__main__":
    root_path = "/Users/leichen/Code/pythonProject/pythonProject/pythonProject/SCRMDetection/experiment/output/result/"
    repos = ["jfinal",
             "mbassador",
             "javapoet",
             "jeromq",
             "seyren",
             "retrolambda",
             "baasbox",
             "sshj",
             "xabber-android",
             "android-async-http",
             "giraph",
             "spring-data-rest",
             "blueflood",
             "HikariCP",
             "redisson",
             "goclipse",
             # "atomix",
             "morphia",
             "PocketHub",
             "hydra",
             "cascading",
             # "robovm",
             "helios",
             "RoboBinding",
             "truth",
             # "assertj",
             "rest.li",
             "rest-assured",
             "JGroups",
             "processing",
             # "spring-framework",
             "zuul"
             ]

    # 2024/11/5
    repos = ["refactoring-toy-example"]

    print(len(repos))
    print(len(set(repos)))

    CGR_count = {}

    # repos = ["javapoet"]
    # repos = ["helios"]
    # repos = ["retrolambda", "JGroups", "zuul", "blueflood"]
    # repos = ["retrolambda"]
    # repos = ["goclipse"]
    # zuul x, retrolambda x, javapoet x, goclipse x, helios 4, mbassador x, jeromq x, jfinal x,
    # repos = ["RoboBinding"]

    res = dict()
    for repo in repos:
        if "_cr" not in repo:
            repo = repo + "_cr"
        repo_path = root_path + repo

        # detect CGR and exclude the CGRs detected from lower-granularity
        all_CGRs, frequency, CGR_num = extract_CGR_filter_sub(repo_path)
        all_CGRs_list = [each for ref in all_CGRs for each in ref]
        all_CGRs_list_str = [str(each) for each in all_CGRs_list]
        res.update(frequency)
        print(repo, end=" ")
        print(frequency)

        CGR_count[repo] = CGR_num
        print(CGR_count)

    # detect CGR but not exclude the CGRs detected from lower-granularity
    # all_CGRs_no_filter, frequency_no_filter = extract_CGR_no_filter_sub(repo_path)
    # res.update(frequency_no_filter)
    # print(repo, end=" ")
    # print(frequency_no_filter)

    # calculate the ratio of override
    #     res[repo] = calculate_override_ratio(repo_path)

    # print(res)

    # with open("./RQ1_frequency_filter.json", "w") as f:
    # with open("./RQ1_frequency_no_filter.json", "w") as f:
    #     json.dump(res, f)

    # with open("dataset_info/CGR_count.json","w") as f:
    #     json.dump(CGR_count, f)

    # override_ratio()

    # # of CGRs per normal commit
    CGR_frequency_per_commit()
    # # of CGRs per refactoring
    CGR_frequency_per_refactoring()
